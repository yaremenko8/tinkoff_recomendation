{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "2020_hack_baseline_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuFnchSfGdd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0439e765-658f-4642-8cc4-33b98973c354"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNkDouT4CfMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cd91eeb-d8d9-4c39-9169-70bad9357dca"
      },
      "source": [
        "! pip install --upgrade numpy pandas tqdm torch catalyst==20.09"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 227kB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/11/e1f53db0614f2721027aab297c8afd2eaf58d33d566441a97ea454541c5e/pandas-1.1.2-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 41.6MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hRequirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Collecting catalyst==20.09\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/18/ed836ed785bd781f34bc006138a6e0d8f91422586a0dbfb7ba4f5343e34f/catalyst-20.9-py2.py3-none-any.whl (452kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting deprecation\n",
            "  Downloading https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (0.22.2.post1)\n",
            "Collecting GitPython>=3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/bc/ae32e07e89cc25b9e5c793d19a1e5454d30a8e37d95040991160f942519e/GitPython-3.1.8-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->catalyst==20.09) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst==20.09) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst==20.09) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst==20.09) (0.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (3.2.2)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst==20.09) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst==20.09) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst==20.09) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.09) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.09) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.09) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.09) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 1.1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, pandas, tqdm, deprecation, tensorboardX, smmap, gitdb, GitPython, catalyst\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed GitPython-3.1.8 catalyst-20.9 deprecation-2.1.0 gitdb-4.0.5 numpy-1.19.2 pandas-1.1.2 smmap-3.0.4 tensorboardX-2.1 tqdm-4.49.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08k0IFOCfM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from bisect import bisect_left, bisect_right\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GPU hack if you need\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD0TKDyMCfM7",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "Columns\n",
        "- `party_rk` – client unique identifier\n",
        "- `account_rk` – client account unique identifier\n",
        "- `financial_account_type_cd` – debit/credit card flag\n",
        "- `transaction_dttm` – operation datetime\n",
        "- `transaction_type_desc` – purchase/payment/...\n",
        "- `transaction_amt_rur` – transaction price\n",
        "- `merchant_type` - DUTY FREE STORES/FUEL DEALERS/RESTAURANTS/ etc\n",
        "- `merchant_group_rk` - McDonald's/Wildberries/ etc\n",
        "\n",
        "It's important that table is already sorted by `transaction_dttm` column!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NxRbuZnfCfM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = \"/content/drive/My Drive/datasets/2009-skoltech-hack/data\" # \"./data\"\n",
        "transactions_path = f\"{DATADIR}/avk_hackathon_data_transactions.csv\"\n",
        "pd.read_csv(f\"{DATADIR}/avk_hackathon_data_transactions.csv\", nrows=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT0HZR2_CfNA",
        "colab_type": "text"
      },
      "source": [
        "## Mappings\n",
        "~1 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKHwvxB5CfNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare & save mappings\n",
        "mappings = defaultdict(dict)\n",
        "unk_token = \"<UNK>\"\n",
        "\n",
        "\n",
        "def create_mapping(values):\n",
        "    mapping = {unk_token: 0}\n",
        "    for v in values:\n",
        "        if not pd.isna(v):\n",
        "            mapping[str(v)] = len(mapping)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "for col in tqdm(\n",
        "    [\n",
        "        \"transaction_type_desc\",\n",
        "        \"merchant_rk\",\n",
        "        \"merchant_type\",\n",
        "        \"merchant_group_rk\",\n",
        "        \"category\",\n",
        "        \"financial_account_type_cd\",\n",
        "    ]\n",
        "):\n",
        "\n",
        "    col_values = (\n",
        "        pd.read_csv(transactions_path, usecols=[col])[col]\n",
        "        .fillna(unk_token)\n",
        "        .astype(str)\n",
        "    )\n",
        "    mappings[col] = create_mapping(col_values.unique())\n",
        "    del col_values\n",
        "\n",
        "\n",
        "with open(f\"{DATADIR}/mappings.json\", \"w\") as f:\n",
        "    json.dump(mappings, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_2kzArrCfND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mappings\n",
        "# with open(f\"{DATADIR}/mappings.json\", 'r') as f:\n",
        "#     mappings = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc69QUTfCfNG",
        "colab_type": "text"
      },
      "source": [
        "## Parse transactions by users\n",
        "~ 40 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ezUae-0CfNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare & save client data\n",
        "party2dates = defaultdict(list)  # for each party save a series of the transaction dates \n",
        "party2sum = defaultdict(list)  # for each party save a series of the transaction costs \n",
        "party2merchant_type = defaultdict(list)  # for each party save a series of the transaction_type \n",
        "party2trans_type = defaultdict(list)  # for each party save a series of the transaction merchant_type\n",
        "\n",
        "usecols = [\n",
        "    \"party_rk\",\n",
        "    \"transaction_dttm\",\n",
        "    \"transaction_amt_rur\",\n",
        "    \"merchant_type\",\n",
        "    \"transaction_type_desc\",\n",
        "]\n",
        "\n",
        "for chunk in tqdm(\n",
        "    pd.read_csv(transactions_path, usecols=usecols, chunksize=100_000)\n",
        "):\n",
        "\n",
        "    chunk[\"merchant_type\"] = (\n",
        "        chunk[\"merchant_type\"].fillna(unk_token).astype(str)\n",
        "    )\n",
        "    chunk[\"transaction_type_desc\"] = (\n",
        "        chunk[\"transaction_type_desc\"].fillna(unk_token).astype(str)\n",
        "    )\n",
        "    chunk[\"transaction_amt_rur\"] = chunk[\"transaction_amt_rur\"].fillna(0)\n",
        "\n",
        "    for i, row in chunk.iterrows():\n",
        "        party2dates[row.party_rk].append(row.transaction_dttm)\n",
        "        party2sum[row.party_rk].append(row.transaction_amt_rur)\n",
        "        party2merchant_type[row.party_rk].append(\n",
        "            mappings[\"merchant_type\"][row.merchant_type]\n",
        "        )\n",
        "        party2trans_type[row.party_rk].append(\n",
        "            mappings[\"transaction_type_desc\"][row.transaction_type_desc]\n",
        "        )\n",
        "\n",
        "    del chunk\n",
        "\n",
        "pickle.dump(party2dates, open(f\"{DATADIR}/party2dates.pkl\", \"wb\"))\n",
        "pickle.dump(party2sum, open(f\"{DATADIR}/party2sum.pkl\", \"wb\"))\n",
        "pickle.dump(party2merchant_type, open(f\"{DATADIR}/party2merchant_type.pkl\", \"wb\"))\n",
        "pickle.dump(party2trans_type, open(f\"{DATADIR}/party2trans_type.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQfeDGZZCfNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load client data\n",
        "# party2dates = pickle.load(open(f\"{DATADIR}/party2dates.pkl\", 'rb'))\n",
        "# party2sum = pickle.load(open(f\"{DATADIR}/party2sum.pkl\", 'rb'))\n",
        "# party2merchant_type = pickle.load(open(f\"{DATADIR}/party2merchant_type.pkl\", 'rb'))\n",
        "# party2trans_type = pickle.load(open(f\"{DATADIR}/party2trans_type.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtLQTSfDCfNL",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf2Pmq0FCfNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15431c89-e221-449a-d864-6aa348921ca7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_party, valid_party = train_test_split(\n",
        "    pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique(), \n",
        "    train_size=0.8, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Train: {len(train_party)} Val: {len(valid_party)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 39994 Val: 9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CGZuEgEoCfNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_period_len = 60  # -- days\n",
        "train_predict_dates = (\n",
        "    pd.date_range(\"2019-03-01\", \"2019-10-31\", freq=\"MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")\n",
        "valid_predict_dates = (\n",
        "    pd.date_range(\"2019-11-01\", \"2019-12-31\", freq=\"MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")\n",
        "submission_predict_dates = (\n",
        "    pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"2MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6rAD6SdCfNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(party_list, mode=\"train\"):\n",
        "    \"\"\"\n",
        "    This function define the pipeline of the creation of train and valid samples.\n",
        "    We consider each client from party_list. For each client take each \n",
        "    predict_period_start from predict_dates list. All client transaction before\n",
        "    this date is our features. Next, we look at the customer's transactions in \n",
        "    the next two months. This transactions should be predicted. It will form \n",
        "    our labels vector.\n",
        "    \"\"\"\n",
        "\n",
        "    data_sum = []\n",
        "    data_trans_type = []\n",
        "    data_merchant_type = []\n",
        "    data_labels = []\n",
        "\n",
        "    for party_rk in tqdm(party_list):\n",
        "        date_series = party2dates[party_rk]\n",
        "        sum_series = party2sum[party_rk]\n",
        "        merch_type_series = party2merchant_type[party_rk]\n",
        "        trans_type_series = party2trans_type[party_rk]\n",
        "\n",
        "        if mode == \"train\":\n",
        "            predict_dates = train_predict_dates\n",
        "        elif mode == \"valid\":\n",
        "            predict_dates = valid_predict_dates\n",
        "        elif mode == \"submission\":\n",
        "            predict_dates = submission_predict_dates\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode\")\n",
        "\n",
        "        for predict_period_start in predict_dates:\n",
        "\n",
        "            predict_period_end = datetime.strftime(\n",
        "                datetime.strptime(predict_period_start, \"%Y-%m-%d\")\n",
        "                + timedelta(days=predict_period_len),\n",
        "                \"%Y-%m-%d\",\n",
        "            )\n",
        "\n",
        "            l, r = (\n",
        "                bisect_left(date_series, predict_period_start),\n",
        "                bisect_right(date_series, predict_period_end),\n",
        "            )\n",
        "\n",
        "            history_merch_type = merch_type_series[:l]\n",
        "            history_sum = sum_series[:l]\n",
        "            history_trans_type = trans_type_series[:l]\n",
        "            predict_merch = merch_type_series[l:r]\n",
        "\n",
        "            if predict_merch and l or mode not in (\"train\", \"valid\"):\n",
        "                data_sum.append(history_sum)\n",
        "                data_trans_type.append(history_trans_type)\n",
        "                data_merchant_type.append(history_merch_type)\n",
        "                data_labels.append(predict_merch)\n",
        "\n",
        "    return data_sum, data_trans_type, data_merchant_type, data_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZzpIPNNCfNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5ffbefb-272d-4ca1-f933-3a0ec422f5a9"
      },
      "source": [
        "train_sum, train_trans_type, train_merchant_type, train_labels = prepare_data(\n",
        "    train_party, mode=\"train\"\n",
        ")\n",
        "valid_sum, valid_trans_type, valid_merchant_type, valid_labels = prepare_data(\n",
        "    valid_party, mode=\"valid\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39994/39994 [00:11<00:00, 3499.09it/s]\n",
            "100%|██████████| 9999/9999 [00:02<00:00, 4903.98it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xdScilCfNW",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gUqipggCfNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8SKiPqCfNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MERCH_TYPE_NCLASSES = len(mappings['merchant_type'])\n",
        "TRANS_TYPE_NCLASSES = len(mappings['transaction_type_desc'])\n",
        "PADDING_LEN = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjllhPWGCfNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RSDataset(Dataset):\n",
        "    def __init__(self, data_sum, data_trans_type, data_merchant_type, labels):\n",
        "        super(RSDataset, self).__init__()\n",
        "        self.data_sum = data_sum\n",
        "        self.data_trans_type = data_trans_type\n",
        "        self.data_merchant_type = data_merchant_type\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_sum)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        targets = np.zeros((MERCH_TYPE_NCLASSES - 1,), dtype=np.float32)\n",
        "        for m in self.labels[idx]:\n",
        "            if m:  # skip UNK, UNK-token should not be predicted\n",
        "                targets[m - 1] = 1.0\n",
        "\n",
        "        item = {\n",
        "            \"features\": {},\n",
        "            \"targets\": targets,\n",
        "        }\n",
        "\n",
        "        sum_feature = np.array(self.data_sum[idx][-PADDING_LEN:])\n",
        "        sum_feature = np.vectorize(lambda s: np.log(1 + s))(sum_feature)\n",
        "        if sum_feature.shape[0] < PADDING_LEN:\n",
        "            pad = np.zeros(\n",
        "                (PADDING_LEN - sum_feature.shape[0],), dtype=np.float32\n",
        "            )\n",
        "            sum_feature = np.hstack((sum_feature, pad))\n",
        "        item[\"features\"][\"sum\"] = torch.from_numpy(sum_feature).float()\n",
        "\n",
        "        for feature_name, feature_values in zip(\n",
        "            [\"trans_type\", \"merchant_type\"],\n",
        "            [self.data_trans_type[idx], self.data_merchant_type[idx]],\n",
        "        ):\n",
        "\n",
        "            feature_values = np.array(feature_values[-PADDING_LEN:])\n",
        "            mask = np.ones(feature_values.shape[0], dtype=np.float32)\n",
        "            if feature_values.shape[0] < PADDING_LEN:\n",
        "                feature_values = np.append(\n",
        "                    feature_values,\n",
        "                    np.zeros(\n",
        "                        PADDING_LEN - feature_values.shape[0], dtype=np.int64\n",
        "                    ),\n",
        "                )\n",
        "                mask = np.append(\n",
        "                    mask,\n",
        "                    np.zeros(PADDING_LEN - mask.shape[0], dtype=np.float32),\n",
        "                )\n",
        "            item[\"features\"][feature_name] = torch.from_numpy(feature_values).long()\n",
        "            item[\"features\"][f\"{feature_name}_mask\"] = torch.from_numpy(mask).float()\n",
        "\n",
        "        return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3-aV7lJCfNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = RSDataset(\n",
        "    train_sum, train_trans_type, train_merchant_type, train_labels\n",
        ")\n",
        "valid_dataset = RSDataset(\n",
        "    valid_sum, valid_trans_type, valid_merchant_type, valid_labels\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8jnB23BCfNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=64, shuffle=False, num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SGRUJWR3CfNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sanity check\n",
        "# for i in tqdm(range(len(train_loader))):\n",
        "#     batch = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J67Ms2_jCfNk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwwVKAh3AUx8",
        "colab_type": "text"
      },
      "source": [
        "This is the baseline model for predicting purchases in `merchant_type` in the next 2 months"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kWquDGFCfNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-PAEoqoCfNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'merchant_type_emb_dim': 64,\n",
        "    'trans_type_embedding': 3,\n",
        "    'transformer_nhead': 2,\n",
        "    'transformer_dim_feedforward': 256,\n",
        "    'transformer_dropout': 0.1,\n",
        "    'dense_unit': 256,\n",
        "    'num_layers': 4,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeAfj7h_CfNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99271840-d606-4a6b-d976-a57c57663c0d"
      },
      "source": [
        "MERCH_TYPE_NCLASSES, TRANS_TYPE_NCLASSES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDhr4svJCfNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.merchant_type_embedding = nn.Embedding(\n",
        "            MERCH_TYPE_NCLASSES, params[\"merchant_type_emb_dim\"]\n",
        "        )\n",
        "        self.trans_type_embedding = nn.Embedding(\n",
        "            TRANS_TYPE_NCLASSES, params[\"trans_type_embedding\"]\n",
        "        )\n",
        "\n",
        "        embedding_size = (\n",
        "            params[\"merchant_type_emb_dim\"]\n",
        "            + params[\"trans_type_embedding\"]\n",
        "            + 1\n",
        "        )\n",
        "\n",
        "        transformer_blocks = []\n",
        "        for i in range(params[\"num_layers\"]):\n",
        "            transformer_block = nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_size,\n",
        "                nhead=params[\"transformer_nhead\"],\n",
        "                dim_feedforward=params[\"transformer_dim_feedforward\"],\n",
        "                dropout=params[\"transformer_dropout\"],\n",
        "            )\n",
        "            transformer_blocks.append(\n",
        "                (f\"transformer_block_{i}\", transformer_block)\n",
        "            )\n",
        "\n",
        "        self.transformer_encoder = nn.Sequential(\n",
        "            OrderedDict(transformer_blocks)\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_size, out_features=params[\"dense_unit\"]\n",
        "        )\n",
        "        self.scorer = nn.Linear(\n",
        "            in_features=params[\"dense_unit\"],\n",
        "            out_features=MERCH_TYPE_NCLASSES - 1,\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "\n",
        "        merchant_type_emb = self.merchant_type_embedding(features[\"merchant_type\"])\n",
        "        trans_type_emb = self.trans_type_embedding(features[\"trans_type\"])\n",
        "\n",
        "        merchant_type_emb = merchant_type_emb * features[\"merchant_type_mask\"].unsqueeze(-1)\n",
        "        trans_type_emb = trans_type_emb * features[\"trans_type_mask\"].unsqueeze(-1)\n",
        "\n",
        "        embeddings = torch.cat(\n",
        "            (merchant_type_emb, trans_type_emb, features[\"sum\"].unsqueeze(-1)),\n",
        "            dim=-1,\n",
        "        )\n",
        "\n",
        "        transformer_output = self.transformer_encoder(embeddings)\n",
        "        pooling = torch.mean(transformer_output, dim=1)\n",
        "        linear = torch.tanh(self.linear(pooling))\n",
        "        merch_logits = self.scorer(linear)\n",
        "\n",
        "        return merch_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NszoLAhbCfNu",
        "colab_type": "text"
      },
      "source": [
        "### One-batch-check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVdmq86KCfNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a98d1094-7aaf-46ca-a49d-b731fe8a8127"
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "batch = next(iter(train_loader))\n",
        "output = model(batch['features'])\n",
        "loss = criterion(output, batch['targets'])\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7022, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZz71tEDCfNw",
        "colab_type": "text"
      },
      "source": [
        "## Train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "[A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcF4frWZCfNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catalyst import dl, utils\n",
        "from catalyst.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6EBSFvEwpQl",
        "colab_type": "text"
      },
      "source": [
        "## Custom metrics for this hackathon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhGkjT0CfNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from catalyst.utils.metrics.functional import preprocess_multi_label_metrics\n",
        "from catalyst.utils.torch import get_activation_fn\n",
        "\n",
        "\n",
        "def multi_label_metrics(\n",
        "    outputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    threshold: Union[float, torch.Tensor],\n",
        "    activation: Optional[str] = None,\n",
        "    eps: float = 1e-7,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Computes multi-label precision for the specified activation and threshold.\n",
        "\n",
        "    Args:\n",
        "        outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
        "            indicates the probability of the example belonging to each of\n",
        "            the K classes, according to the model.\n",
        "        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
        "            classes are associated with the N-th input\n",
        "            (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "            associated with classes 2 and 4)\n",
        "        threshold (float): threshold for for model output\n",
        "        activation (str): activation to use for model output\n",
        "        eps (float): epsilon to avoid zero division\n",
        "    \n",
        "    Extended version of \n",
        "        https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/metrics/accuracy.py#L58\n",
        "\n",
        "    Returns:\n",
        "        computed multi-label metrics\n",
        "    \"\"\"\n",
        "    outputs, targets, _ = preprocess_multi_label_metrics(\n",
        "        outputs=outputs, targets=targets\n",
        "    )\n",
        "    activation_fn = get_activation_fn(activation)\n",
        "    outputs = activation_fn(outputs)\n",
        "\n",
        "    outputs = (outputs > threshold).long()\n",
        "\n",
        "    accuracy = (targets.long() == outputs.long()).sum().float() / np.prod(\n",
        "        targets.shape\n",
        "    )\n",
        "\n",
        "    intersection = (outputs.long() * targets.long()).sum(axis=1).float()\n",
        "    num_predicted = outputs.long().sum(axis=1).float()\n",
        "    num_relevant = targets.long().sum(axis=1).float()\n",
        "    union = num_predicted + num_relevant\n",
        "\n",
        "    # Precision = ({predicted items} && {relevant items}) / {predicted items}\n",
        "    precision = intersection / (num_predicted + eps * (num_predicted == 0))\n",
        "    # Recall = ({predicted items} && {relevant items}) / {relevant items}\n",
        "    recall = intersection / (num_relevant + eps * (num_relevant == 0))\n",
        "    # IoU = ({predicted items} && {relevant items}) / ({predicted items} || {relevant items})\n",
        "    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n",
        "\n",
        "    return accuracy, precision.mean(), recall.mean(), iou.mean()\n",
        "\n",
        "\n",
        "def precision_at_k(\n",
        "    actual: torch.Tensor, \n",
        "    predicted: torch.Tensor, \n",
        "    k: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes precision at cutoff k for one sample\n",
        "\n",
        "    Args:\n",
        "       actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
        "       predicted (torch.Tensor): binary tensor that encodes which of the K\n",
        "           classes are associated with the N-th input\n",
        "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "           associated with classes 2 and 4)\n",
        "       k (int): parameter k of precison@k\n",
        "\n",
        "    Returns:\n",
        "       Computed value of precision@k for given sample\n",
        "    \"\"\"\n",
        "    p_at_k = 0.0\n",
        "    for item in predicted[:k]:\n",
        "        if actual[item]:\n",
        "            p_at_k += 1\n",
        "    p_at_k /= k\n",
        "\n",
        "    return p_at_k\n",
        "\n",
        "\n",
        "def average_precision_at_k(\n",
        "    actual: torch.Tensor, \n",
        "    predicted: torch.Tensor, \n",
        "    k: int,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes average precision at cutoff k for one sample\n",
        "\n",
        "    Args:\n",
        "      actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
        "      predicted (torch.Tensor): binary tensor that encodes which of the K\n",
        "          classes are associated with the N-th input\n",
        "          (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "          associated with classes 2 and 4)\n",
        "      k (int): parameter k of AP@k\n",
        "\n",
        "    Returns:\n",
        "        Computed value of AP@k for given sample\n",
        "    \"\"\"\n",
        "    ap_at_k = 0.0\n",
        "    for idx, item in enumerate(predicted[:k]):\n",
        "        if actual[item]:\n",
        "            ap_at_k += precision_at_k(actual, predicted, k=idx + 1)\n",
        "    ap_at_k /= min(k, actual.sum().cpu().numpy())\n",
        "    \n",
        "\n",
        "    return ap_at_k\n",
        "\n",
        "\n",
        "def mean_average_precision_at_k(\n",
        "    output: torch.Tensor, target: torch.Tensor, top_k: Tuple[int, ...] = (1,)\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes mean_average_precision_at_k at set of cutoff parameters K\n",
        "\n",
        "    Args:\n",
        "       outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
        "           indicates the probability of the example belonging to each of\n",
        "           the K classes, according to the model.\n",
        "       targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
        "           classes are associated with the N-th input\n",
        "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "           associated with classes 2 and 4)\n",
        "       top_k (tuple): list of parameters k at which map@k will be computed\n",
        "\n",
        "\n",
        "    Returns:\n",
        "       List of computed values of map@k at each cutoff k from topk\n",
        "    \"\"\"\n",
        "    max_k = max(top_k)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, top_indices = output.topk(k=max_k, dim=1, largest=True, sorted=True)\n",
        "\n",
        "    result = []\n",
        "    for k in top_k:  # loop over k\n",
        "        map_at_k = 0.0\n",
        "        for actual_target, predicted_items in zip(\n",
        "            target, top_indices\n",
        "        ):  # loop over samples\n",
        "            map_at_k += average_precision_at_k(\n",
        "                actual_target, predicted_items, k\n",
        "            )\n",
        "        map_at_k = map_at_k / batch_size\n",
        "        result.append(map_at_k)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygG0d1WCfN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is Runner?\n",
        "# https://catalyst-team.github.io/catalyst/api/core.html#runner\n",
        "class CustomRunner(dl.Runner):\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        # model train/valid step\n",
        "        features, targets = batch[\"features\"], batch[\"targets\"]\n",
        "        logits = self.model(features)\n",
        "        scores = torch.sigmoid(logits)\n",
        "\n",
        "        loss = self.criterion(logits, targets)\n",
        "        accuracy, precision, recall, iou = multi_label_metrics(\n",
        "            logits, targets, threshold=0.5, activation=\"Sigmoid\"\n",
        "        )\n",
        "        map05, map10, map20 = mean_average_precision_at_k(\n",
        "            scores, targets, top_k=(5, 10, 20)\n",
        "        )\n",
        "        batch_metrics = {\n",
        "            \"loss\": loss,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"iou\": iou,\n",
        "            \"map05\": map05,\n",
        "            \"map10\": map10,\n",
        "            \"map20\": map20\n",
        "        }\n",
        "        \n",
        "        self.input = {\"features\": features, \"targets\": targets}\n",
        "        self.output = {\"logits\": logits, \"scores\": scores}\n",
        "        self.batch_metrics.update(batch_metrics)\n",
        "\n",
        "        if self.is_train_loader:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "    \n",
        "    def predict_batch(self, batch):\n",
        "        # model inference step\n",
        "        batch = utils.maybe_recursive_call(batch, \"to\", device=self.device)\n",
        "        logits = self.model(batch[\"features\"])\n",
        "        scores = torch.sigmoid(logits)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noTAPVPSCfN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-bnRkkSRYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhxJPaHTCfN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "f761ac29-ebf8-4292-e6ff-a1b99c524774"
      },
      "source": [
        "# For other minimal examples, please follow the link below\n",
        "# https://github.com/catalyst-team/catalyst#minimal-examples\n",
        "runner = CustomRunner()\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=None,\n",
        "    loaders=loaders,\n",
        "    logdir=\"./logs\",\n",
        "    num_epochs=10,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        "    overfit=False,  #  <<<--- DO NOT FORGET TO MAKE IT ``False`` \n",
        "                    #  (``True`` uses only one batch to check pipeline correctness)\n",
        "    callbacks=[\n",
        "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
        "        # dl.AveragePrecisionCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"ap\"),\n",
        "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "        # dl.AUCCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"auc\"),\n",
        "    ],\n",
        "    main_metric=\"iou\", # \"ap/mean\", \n",
        "    minimize_metric=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10 * Epoch (train): 100% 3258/3258 [20:05<00:00,  2.70it/s, accuracy=0.984, iou=0.260, loss=0.047, map05=0.585, map10=0.536, map20=0.518, precision=0.693, recall=0.307]\n",
            "1/10 * Epoch (valid): 100% 253/253 [01:24<00:00,  2.99it/s, accuracy=0.983, iou=0.280, loss=0.056, map05=0.589, map10=0.451, map20=0.399, precision=0.665, recall=0.370]\n",
            "[2020-09-17 11:36:25,117] \n",
            "1/10 * Epoch 1 (train): accuracy=0.9836 | iou=0.2356 | loss=0.0504 | map05=0.5561 | map10=0.5043 | map20=0.4941 | precision=0.6421 | recall=0.2976\n",
            "1/10 * Epoch 1 (valid): accuracy=0.9845 | iou=0.2979 | loss=0.0472 | map05=0.6398 | map10=0.5723 | map20=0.5515 | precision=0.6590 | recall=0.3923\n",
            "2/10 * Epoch (train): 100% 3258/3258 [20:14<00:00,  2.68it/s, accuracy=0.988, iou=0.328, loss=0.040, map05=0.621, map10=0.574, map20=0.573, precision=0.695, recall=0.450]\n",
            "2/10 * Epoch (valid): 100% 253/253 [01:24<00:00,  2.98it/s, accuracy=0.982, iou=0.282, loss=0.054, map05=0.604, map10=0.480, map20=0.442, precision=0.631, recall=0.377]\n",
            "[2020-09-17 11:58:04,786] \n",
            "2/10 * Epoch 2 (train): accuracy=0.9850 | iou=0.2982 | loss=0.0454 | map05=0.6499 | map10=0.5862 | map20=0.5681 | precision=0.6889 | recall=0.3823\n",
            "2/10 * Epoch 2 (valid): accuracy=0.9852 | iou=0.3384 | loss=0.0453 | map05=0.6858 | map10=0.6188 | map20=0.5960 | precision=0.7003 | recall=0.4466\n",
            "3/10 * Epoch (train): 100% 3258/3258 [20:23<00:00,  2.66it/s, accuracy=0.986, iou=0.333, loss=0.044, map05=0.692, map10=0.624, map20=0.591, precision=0.732, recall=0.413]\n",
            "3/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.94it/s, accuracy=0.984, iou=0.292, loss=0.053, map05=0.620, map10=0.522, map20=0.450, precision=0.646, recall=0.396]\n",
            "[2020-09-17 12:19:54,062] \n",
            "3/10 * Epoch 3 (train): accuracy=0.9855 | iou=0.3264 | loss=0.0440 | map05=0.6802 | map10=0.6166 | map20=0.5965 | precision=0.7118 | recall=0.4220\n",
            "3/10 * Epoch 3 (valid): accuracy=0.9855 | iou=0.3469 | loss=0.0442 | map05=0.7044 | map10=0.6377 | map20=0.6127 | precision=0.7131 | recall=0.4549\n",
            "4/10 * Epoch (train): 100% 3258/3258 [20:24<00:00,  2.66it/s, accuracy=0.986, iou=0.335, loss=0.044, map05=0.729, map10=0.624, map20=0.617, precision=0.689, recall=0.510]\n",
            "4/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.92it/s, accuracy=0.982, iou=0.272, loss=0.053, map05=0.612, map10=0.527, map20=0.451, precision=0.578, recall=0.387]\n",
            "[2020-09-17 12:41:45,111] \n",
            "4/10 * Epoch 4 (train): accuracy=0.9857 | iou=0.3419 | loss=0.0432 | map05=0.6932 | map10=0.6311 | map20=0.6106 | precision=0.7174 | recall=0.4447\n",
            "4/10 * Epoch 4 (valid): accuracy=0.9855 | iou=0.3688 | loss=0.0436 | map05=0.7094 | map10=0.6458 | map20=0.6219 | precision=0.6783 | recall=0.5162\n",
            "5/10 * Epoch (train): 100% 3258/3258 [20:34<00:00,  2.64it/s, accuracy=0.985, iou=0.347, loss=0.045, map05=0.697, map10=0.605, map20=0.586, precision=0.697, recall=0.449]\n",
            "5/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.91it/s, accuracy=0.983, iou=0.291, loss=0.053, map05=0.622, map10=0.524, map20=0.442, precision=0.621, recall=0.401]\n",
            "[2020-09-17 13:03:46,952] \n",
            "5/10 * Epoch 5 (train): accuracy=0.9858 | iou=0.3507 | loss=0.0427 | map05=0.7000 | map10=0.6393 | map20=0.6188 | precision=0.7172 | recall=0.4587\n",
            "5/10 * Epoch 5 (valid): accuracy=0.9857 | iou=0.3683 | loss=0.0429 | map05=0.7175 | map10=0.6544 | map20=0.6295 | precision=0.7000 | recall=0.4998\n",
            "6/10 * Epoch (train): 100% 3258/3258 [20:48<00:00,  2.61it/s, accuracy=0.987, iou=0.350, loss=0.040, map05=0.707, map10=0.642, map20=0.629, precision=0.718, recall=0.476]\n",
            "6/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.91it/s, accuracy=0.984, iou=0.315, loss=0.052, map05=0.622, map10=0.538, map20=0.471, precision=0.617, recall=0.428]\n",
            "[2020-09-17 13:26:01,960] \n",
            "6/10 * Epoch 6 (train): accuracy=0.9859 | iou=0.3567 | loss=0.0423 | map05=0.7041 | map10=0.6445 | map20=0.6243 | precision=0.7173 | recall=0.4683\n",
            "6/10 * Epoch 6 (valid): accuracy=0.9858 | iou=0.3720 | loss=0.0428 | map05=0.7149 | map10=0.6537 | map20=0.6288 | precision=0.6953 | recall=0.5087\n",
            "7/10 * Epoch (train): 100% 3258/3258 [20:40<00:00,  2.63it/s, accuracy=0.986, iou=0.345, loss=0.041, map05=0.701, map10=0.652, map20=0.630, precision=0.701, recall=0.454]\n",
            "7/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.89it/s, accuracy=0.984, iou=0.301, loss=0.050, map05=0.649, map10=0.569, map20=0.487, precision=0.620, recall=0.410]\n",
            "[2020-09-17 13:48:09,658] \n",
            "7/10 * Epoch 7 (train): accuracy=0.9860 | iou=0.3607 | loss=0.0420 | map05=0.7067 | map10=0.6482 | map20=0.6281 | precision=0.7165 | recall=0.4746\n",
            "7/10 * Epoch 7 (valid): accuracy=0.9858 | iou=0.3783 | loss=0.0426 | map05=0.7250 | map10=0.6638 | map20=0.6399 | precision=0.6921 | recall=0.5213\n",
            "8/10 * Epoch (train): 100% 3258/3258 [20:48<00:00,  2.61it/s, accuracy=0.987, iou=0.385, loss=0.040, map05=0.710, map10=0.663, map20=0.637, precision=0.693, recall=0.547]\n",
            "8/10 * Epoch (valid): 100% 253/253 [01:28<00:00,  2.86it/s, accuracy=0.985, iou=0.336, loss=0.050, map05=0.641, map10=0.538, map20=0.464, precision=0.627, recall=0.452]\n",
            "[2020-09-17 14:10:27,181] \n",
            "8/10 * Epoch 8 (train): accuracy=0.9860 | iou=0.3634 | loss=0.0418 | map05=0.7088 | map10=0.6510 | map20=0.6313 | precision=0.7159 | recall=0.4795\n",
            "8/10 * Epoch 8 (valid): accuracy=0.9858 | iou=0.3795 | loss=0.0424 | map05=0.7213 | map10=0.6626 | map20=0.6379 | precision=0.6852 | recall=0.5271\n",
            "9/10 * Epoch (train): 100% 3258/3258 [20:37<00:00,  2.63it/s, accuracy=0.985, iou=0.351, loss=0.044, map05=0.704, map10=0.658, map20=0.627, precision=0.711, recall=0.484]\n",
            "9/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.90it/s, accuracy=0.984, iou=0.303, loss=0.052, map05=0.610, map10=0.538, map20=0.461, precision=0.615, recall=0.420]\n",
            "[2020-09-17 14:32:32,102] \n",
            "9/10 * Epoch 9 (train): accuracy=0.9860 | iou=0.3662 | loss=0.0416 | map05=0.7102 | map10=0.6533 | map20=0.6335 | precision=0.7152 | recall=0.4838\n",
            "9/10 * Epoch 9 (valid): accuracy=0.9859 | iou=0.3771 | loss=0.0422 | map05=0.7231 | map10=0.6651 | map20=0.6419 | precision=0.6996 | recall=0.5110\n",
            "10/10 * Epoch (train): 100% 3258/3258 [20:59<00:00,  2.59it/s, accuracy=0.986, iou=0.380, loss=0.040, map05=0.748, map10=0.697, map20=0.648, precision=0.681, recall=0.511]\n",
            "10/10 * Epoch (valid): 100% 253/253 [01:28<00:00,  2.87it/s, accuracy=0.984, iou=0.288, loss=0.051, map05=0.611, map10=0.503, map20=0.452, precision=0.551, recall=0.359]\n",
            "[2020-09-17 14:54:59,952] \n",
            "10/10 * Epoch 10 (train): accuracy=0.9861 | iou=0.3679 | loss=0.0414 | map05=0.7114 | map10=0.6548 | map20=0.6355 | precision=0.7145 | recall=0.4869\n",
            "10/10 * Epoch 10 (valid): accuracy=0.9858 | iou=0.3814 | loss=0.0423 | map05=0.7228 | map10=0.6638 | map20=0.6404 | precision=0.6859 | recall=0.5298\n",
            "Top best models:\n",
            "logs/checkpoints/train.10.pth\t0.3814\n",
            "=> Loading checkpoint logs/checkpoints/best_full.pth\n",
            "loaded state checkpoint logs/checkpoints/best_full.pth (global epoch 10, epoch 10, stage train)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxuxPukCfN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model inference example\n",
        "# for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
        "#     assert prediction.detach().cpu().numpy().shape[-1] == MERCH_TYPE_NCLASSES-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IFV8lYM9CfN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import plotly  # required for contrib\n",
        "# from catalyst.contrib.utils import plot_tensorboard_log\n",
        "\n",
        "# plot_tensorboard_log(logdir=\"./logs\", step=\"batch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Bte9iiW0CfOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly  # required for contrib\n",
        "from catalyst.contrib.utils import plot_tensorboard_log\n",
        "\n",
        "plot_tensorboard_log(\n",
        "    logdir=\"./logs\", \n",
        "    step=\"epoch\", \n",
        "    metrics=[\n",
        "        \"loss\", \"accuracy\", \"precision\", \"recall\", \"iou\", \n",
        "        \"map05\", \"map10\", \"map20\",\n",
        "        \"ap/mean\", \"auc/mean\"\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOBi1DqJPt_F",
        "colab_type": "text"
      },
      "source": [
        "## Your task\n",
        "\n",
        "We suggest you to improve this baseline. Feel free to use any kind of model architectures, loss functions, inputs, etc. in your experiments.\n",
        "\n",
        "\n",
        "YOUR TASK is to predict purchases in `merchant_type` in **January-February 2020** for all the clients (50k) from the given dataset.\n",
        "\n",
        "SUBMISSION FORMAT: You should submit a `.csv` file in the following format. \n",
        "\n",
        "The submission file should contain two columns:\n",
        "* `party_rk` -- client unique identifier\n",
        "* `recommendations` -- list of the **top 30** predicted `merchant_type`, sorted by predicted proba (pay attention!) **separated by commas**. \n",
        "\n",
        "The `.csv` file separator should be **semicolon (\";\")**. The submission file example can be generated by the pipeline shown below.\n",
        "\n",
        "EVALUATION: Your submission will be evaluated by metric **MAP@30**. Scores for this part of the hackathon will be given according to the value of this metric.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-30pWSiQL7vY",
        "colab_type": "text"
      },
      "source": [
        "## Submission file example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylgn_EIXMQrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create data loader for submission\n",
        "full_party = pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique()\n",
        "full_sum, full_trans_type, full_merchant_type, full_labels = prepare_data(\n",
        "    full_party, mode=\"submission\"\n",
        ")\n",
        "full_dataset = RSDataset(\n",
        "   full_sum, full_trans_type, full_merchant_type, full_labels\n",
        ")\n",
        "full_loader = DataLoader(\n",
        "    full_dataset, batch_size=64, shuffle=False, num_workers=8, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tdEPJ0gMahr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get predictions from the model\n",
        "predictions = []\n",
        "for scores in tqdm(runner.predict_loader(loader=full_loader), total = len(full_loader)):\n",
        "    _, top_indices = scores.topk(k=30, dim=1, largest=True, sorted=True)\n",
        "    top_indices += 1\n",
        "    predictions += top_indices.detach().cpu().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZhfVRYmMpdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inverse mapping for merchant_type in predictions\n",
        "merchant_type_inverse_mapping = {k: v for v, k in mappings['merchant_type'].items()}\n",
        "def inverse_mapping(x):\n",
        "    return list(map(merchant_type_inverse_mapping.get, x))\n",
        "\n",
        "predictions = list(map(inverse_mapping, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_CjmgByL9Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create submission table\n",
        "submission = pd.DataFrame({\n",
        "    \"party_rk\" : full_party, \n",
        "    \"recommendations\" : predictions\n",
        "})\n",
        "submission['recommendations'] = submission['recommendations'].apply(lambda x: \",\".join(map(str, x)))\n",
        "\n",
        "submission.to_csv('submission_[TEAM_NAME].csv', index=False, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}