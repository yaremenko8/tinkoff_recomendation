{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yaremenko8/tinkoff_recomendation/blob/master/2020_hack_baseline_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EhyXMbUFJOzR",
    "outputId": "a546ca30-03d4-4cce-e6c4-a63f5b0795b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/hacklab_data_hack2\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgotten to 'Add a shortcut to Drive'?\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.insert(0, str(AUX_DATA_ROOT))\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qNkDouT4CfMz",
    "outputId": "b601c5e4-8e39-4aac-c059-042ecdc13b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.19.2)\n",
      "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
      "Requirement already up-to-date: tqdm in /usr/local/lib/python3.6/dist-packages (4.49.0)\n",
      "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
      "Requirement already up-to-date: catalyst==20.09 in /usr/local/lib/python3.6/dist-packages (20.9)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: deprecation in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (3.1.8)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: tensorboardX in /usr/local/lib/python3.6/dist-packages (from catalyst==20.09) (2.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst==20.09) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (50.3.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst==20.09) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst==20.09) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst==20.09) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst==20.09) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst==20.09) (1.0.18)\n",
      "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=3.1.1->catalyst==20.09) (4.0.5)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.09) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.09) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst==20.09) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst==20.09) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst==20.09) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst==20.09) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.1->catalyst==20.09) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.09) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.09) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.09) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade numpy pandas tqdm torch catalyst==20.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o08k0IFOCfM3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from bisect import bisect_left, bisect_right\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU hack if you need\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD0TKDyMCfM7"
   },
   "source": [
    "# Data\n",
    "\n",
    "Columns\n",
    "- `party_rk` – client unique identifier\n",
    "- `account_rk` – client account unique identifier\n",
    "- `financial_account_type_cd` – debit/credit card flag\n",
    "- `transaction_dttm` – operation datetime\n",
    "- `transaction_type_desc` – purchase/payment/...\n",
    "- `transaction_amt_rur` – transaction price\n",
    "- `merchant_type` - DUTY FREE STORES/FUEL DEALERS/RESTAURANTS/ etc\n",
    "- `merchant_group_rk` - McDonald's/Wildberries/ etc\n",
    "\n",
    "It's important that table is already sorted by `transaction_dttm` column!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "NxRbuZnfCfM8",
    "outputId": "99ec99f5-6eae-4fc9-e6af-bfa668f603de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_rk</th>\n",
       "      <th>account_rk</th>\n",
       "      <th>financial_account_type_cd</th>\n",
       "      <th>transaction_dttm</th>\n",
       "      <th>transaction_type_desc</th>\n",
       "      <th>transaction_amt_rur</th>\n",
       "      <th>merchant_rk</th>\n",
       "      <th>merchant_type</th>\n",
       "      <th>merchant_group_rk</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20337</td>\n",
       "      <td>19666</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>84.00</td>\n",
       "      <td>88676.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сувениры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63404</td>\n",
       "      <td>72991</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>410.00</td>\n",
       "      <td>887248.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>Фаст Фуд</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24789</td>\n",
       "      <td>23517</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>701.44</td>\n",
       "      <td>830014.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Супермаркеты</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57970</td>\n",
       "      <td>64838</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>6203.70</td>\n",
       "      <td>363834.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>Дом/Ремонт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12232</td>\n",
       "      <td>11591</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>734.53</td>\n",
       "      <td>85919.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>Супермаркеты</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   party_rk  account_rk  ...  merchant_group_rk      category\n",
       "0     20337       19666  ...                NaN      Сувениры\n",
       "1     63404       72991  ...              725.0      Фаст Фуд\n",
       "2     24789       23517  ...                NaN  Супермаркеты\n",
       "3     57970       64838  ...              454.0    Дом/Ремонт\n",
       "4     12232       11591  ...              878.0  Супермаркеты\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATADIR = AUX_DATA_ROOT # \"./data\"\n",
    "transactions_path = f\"{DATADIR}/avk_hackathon_data_transactions.csv\"\n",
    "pd.read_csv(f\"{DATADIR}/avk_hackathon_data_transactions.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VT0HZR2_CfNA"
   },
   "source": [
    "## Mappings\n",
    "~1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKHwvxB5CfNA",
    "outputId": "4b3b54f5-a9bc-4e24-c9e5-4e1f35127c40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:26<00:00, 14.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare & save mappings\n",
    "mappings = defaultdict(dict)\n",
    "unk_token = \"<UNK>\"\n",
    "\n",
    "\n",
    "def create_mapping(values):\n",
    "    mapping = {unk_token: 0}\n",
    "    for v in values:\n",
    "        if not pd.isna(v):\n",
    "            mapping[str(v)] = len(mapping)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "for col in tqdm(\n",
    "    [\n",
    "        \"transaction_type_desc\",\n",
    "        \"merchant_rk\",\n",
    "        \"merchant_type\",\n",
    "        \"merchant_group_rk\",\n",
    "        \"category\",\n",
    "        \"financial_account_type_cd\",\n",
    "    ]\n",
    "):\n",
    "\n",
    "    col_values = (\n",
    "        pd.read_csv(transactions_path, usecols=[col])[col]\n",
    "        .fillna(unk_token)\n",
    "        .astype(str)\n",
    "    )\n",
    "    mappings[col] = create_mapping(col_values.unique())\n",
    "    del col_values\n",
    "\n",
    "\n",
    "with open(f\"{DATADIR}/mappings.json\", \"w\") as f:\n",
    "    json.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_2kzArrCfND"
   },
   "outputs": [],
   "source": [
    "# load mappings\n",
    "# with open(f\"{DATADIR}/mappings.json\", 'r') as f:\n",
    "#     mappings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yc69QUTfCfNG"
   },
   "source": [
    "## Parse transactions by users\n",
    "~ 40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ezUae-0CfNG",
    "outputId": "01e0b595-017a-456a-deee-ad2ba31b7e64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [28:02, 14.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare & save client data\n",
    "party2dates = defaultdict(list)  # for each party save a series of the transaction dates \n",
    "party2sum = defaultdict(list)  # for each party save a series of the transaction costs \n",
    "party2merchant_type = defaultdict(list)  # for each party save a series of the transaction_type \n",
    "party2trans_type = defaultdict(list)  # for each party save a series of the transaction merchant_type\n",
    "\n",
    "usecols = [\n",
    "    \"party_rk\",\n",
    "    \"transaction_dttm\",\n",
    "    \"transaction_amt_rur\",\n",
    "    \"merchant_type\",\n",
    "    \"transaction_type_desc\",\n",
    "]\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(transactions_path, usecols=usecols, chunksize=100_000)\n",
    "):\n",
    "\n",
    "    chunk[\"merchant_type\"] = (\n",
    "        chunk[\"merchant_type\"].fillna(unk_token).astype(str)\n",
    "    )\n",
    "    chunk[\"transaction_type_desc\"] = (\n",
    "        chunk[\"transaction_type_desc\"].fillna(unk_token).astype(str)\n",
    "    )\n",
    "    chunk[\"transaction_amt_rur\"] = chunk[\"transaction_amt_rur\"].fillna(0)\n",
    "\n",
    "    for i, row in chunk.iterrows():\n",
    "        party2dates[row.party_rk].append(row.transaction_dttm)\n",
    "        party2sum[row.party_rk].append(row.transaction_amt_rur)\n",
    "        party2merchant_type[row.party_rk].append(\n",
    "            mappings[\"merchant_type\"][row.merchant_type]\n",
    "        )\n",
    "        party2trans_type[row.party_rk].append(\n",
    "            mappings[\"transaction_type_desc\"][row.transaction_type_desc]\n",
    "        )\n",
    "\n",
    "    del chunk\n",
    "\n",
    "pickle.dump(party2dates, open(f\"{DATADIR}/party2dates.pkl\", \"wb\"))\n",
    "pickle.dump(party2sum, open(f\"{DATADIR}/party2sum.pkl\", \"wb\"))\n",
    "pickle.dump(party2merchant_type, open(f\"{DATADIR}/party2merchant_type.pkl\", \"wb\"))\n",
    "pickle.dump(party2trans_type, open(f\"{DATADIR}/party2trans_type.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQfeDGZZCfNJ"
   },
   "outputs": [],
   "source": [
    "# load client data\n",
    "# party2dates = pickle.load(open(f\"{DATADIR}/party2dates.pkl\", 'rb'))\n",
    "# party2sum = pickle.load(open(f\"{DATADIR}/party2sum.pkl\", 'rb'))\n",
    "# party2merchant_type = pickle.load(open(f\"{DATADIR}/party2merchant_type.pkl\", 'rb'))\n",
    "# party2trans_type = pickle.load(open(f\"{DATADIR}/party2trans_type.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtLQTSfDCfNL"
   },
   "source": [
    "## PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uf2Pmq0FCfNM",
    "outputId": "55c647d1-6fbf-4e39-cc8c-384569044112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40000 Val: 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_party, valid_party = train_test_split(\n",
    "    pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique(), \n",
    "    train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_party)} Val: {len(valid_party)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGZuEgEoCfNO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_period_len = 60  # -- days\n",
    "train_predict_dates = (\n",
    "    pd.date_range(\"2019-03-01\", \"2019-10-31\", freq=\"MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "valid_predict_dates = (\n",
    "    pd.date_range(\"2019-11-01\", \"2019-12-31\", freq=\"MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "submission_predict_dates = (\n",
    "    pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"2MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6rAD6SdCfNR"
   },
   "outputs": [],
   "source": [
    "def prepare_data(party_list, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    This function define the pipeline of the creation of train and valid samples.\n",
    "    We consider each client from party_list. For each client take each \n",
    "    predict_period_start from predict_dates list. All client transaction before\n",
    "    this date is our features. Next, we look at the customer's transactions in \n",
    "    the next two months. This transactions should be predicted. It will form \n",
    "    our labels vector.\n",
    "    \"\"\"\n",
    "\n",
    "    data_sum = []\n",
    "    data_trans_type = []\n",
    "    data_merchant_type = []\n",
    "    data_labels = []\n",
    "\n",
    "    for party_rk in tqdm(party_list):\n",
    "        date_series = party2dates[party_rk]\n",
    "        sum_series = party2sum[party_rk]\n",
    "        merch_type_series = party2merchant_type[party_rk]\n",
    "        trans_type_series = party2trans_type[party_rk]\n",
    "\n",
    "        if mode == \"train\":\n",
    "            predict_dates = train_predict_dates\n",
    "        elif mode == \"valid\":\n",
    "            predict_dates = valid_predict_dates\n",
    "        elif mode == \"submission\":\n",
    "            predict_dates = submission_predict_dates\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode\")\n",
    "\n",
    "        for predict_period_start in predict_dates:\n",
    "\n",
    "            predict_period_end = datetime.strftime(\n",
    "                datetime.strptime(predict_period_start, \"%Y-%m-%d\")\n",
    "                + timedelta(days=predict_period_len),\n",
    "                \"%Y-%m-%d\",\n",
    "            )\n",
    "\n",
    "            l, r = (\n",
    "                bisect_left(date_series, predict_period_start),\n",
    "                bisect_right(date_series, predict_period_end),\n",
    "            )\n",
    "\n",
    "            history_merch_type = merch_type_series[:l]\n",
    "            history_sum = sum_series[:l]\n",
    "            history_trans_type = trans_type_series[:l]\n",
    "            predict_merch = merch_type_series[l:r]\n",
    "\n",
    "            if predict_merch and l or mode not in (\"train\", \"valid\"):\n",
    "                data_sum.append(history_sum)\n",
    "                data_trans_type.append(history_trans_type)\n",
    "                data_merchant_type.append(history_merch_type)\n",
    "                data_labels.append(predict_merch)\n",
    "\n",
    "    return data_sum, data_trans_type, data_merchant_type, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zZzpIPNNCfNT",
    "outputId": "e14dbf78-6f30-4e26-fbad-be06c8fa1f5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:18<00:00, 2162.74it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 9772.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sum, train_trans_type, train_merchant_type, train_labels = prepare_data(\n",
    "    train_party, mode=\"train\"\n",
    ")\n",
    "valid_sum, valid_trans_type, valid_merchant_type, valid_labels = prepare_data(\n",
    "    valid_party, mode=\"valid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-xdScilCfNW"
   },
   "source": [
    "## PyTorch loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gUqipggCfNX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex8SKiPqCfNZ"
   },
   "outputs": [],
   "source": [
    "MERCH_TYPE_NCLASSES = len(mappings['merchant_type'])\n",
    "TRANS_TYPE_NCLASSES = len(mappings['transaction_type_desc'])\n",
    "PADDING_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjllhPWGCfNb"
   },
   "outputs": [],
   "source": [
    "class RSDataset(Dataset):\n",
    "    def __init__(self, data_sum, data_trans_type, data_merchant_type, labels):\n",
    "        super(RSDataset, self).__init__()\n",
    "        self.data_sum = data_sum\n",
    "        self.data_trans_type = data_trans_type\n",
    "        self.data_merchant_type = data_merchant_type\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_sum)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        targets = np.zeros((MERCH_TYPE_NCLASSES - 1,), dtype=np.float32)\n",
    "        for m in self.labels[idx]:\n",
    "            if m:  # skip UNK, UNK-token should not be predicted\n",
    "                targets[m - 1] = 1.0\n",
    "\n",
    "        item = {\n",
    "            \"features\": {},\n",
    "            \"targets\": targets,\n",
    "        }\n",
    "\n",
    "        sum_feature = np.array(self.data_sum[idx][-PADDING_LEN:])\n",
    "        sum_feature = np.vectorize(lambda s: np.log(1 + s))(sum_feature)\n",
    "        if sum_feature.shape[0] < PADDING_LEN:\n",
    "            pad = np.zeros(\n",
    "                (PADDING_LEN - sum_feature.shape[0],), dtype=np.float32\n",
    "            )\n",
    "            sum_feature = np.hstack((sum_feature, pad))\n",
    "        item[\"features\"][\"sum\"] = torch.from_numpy(sum_feature).float()\n",
    "\n",
    "        for feature_name, feature_values in zip(\n",
    "            [\"trans_type\", \"merchant_type\"],\n",
    "            [self.data_trans_type[idx], self.data_merchant_type[idx]],\n",
    "        ):\n",
    "\n",
    "            feature_values = np.array(feature_values[-PADDING_LEN:])\n",
    "            mask = np.ones(feature_values.shape[0], dtype=np.float32)\n",
    "            if feature_values.shape[0] < PADDING_LEN:\n",
    "                feature_values = np.append(\n",
    "                    feature_values,\n",
    "                    np.zeros(\n",
    "                        PADDING_LEN - feature_values.shape[0], dtype=np.int64\n",
    "                    ),\n",
    "                )\n",
    "                mask = np.append(\n",
    "                    mask,\n",
    "                    np.zeros(PADDING_LEN - mask.shape[0], dtype=np.float32),\n",
    "                )\n",
    "            item[\"features\"][feature_name] = torch.from_numpy(feature_values).long()\n",
    "            item[\"features\"][f\"{feature_name}_mask\"] = torch.from_numpy(mask).float()\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3-aV7lJCfNd"
   },
   "outputs": [],
   "source": [
    "train_dataset = RSDataset(\n",
    "    train_sum, train_trans_type, train_merchant_type, train_labels\n",
    ")\n",
    "valid_dataset = RSDataset(\n",
    "    valid_sum, valid_trans_type, valid_merchant_type, valid_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8jnB23BCfNg"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=64, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGRUJWR3CfNi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "# for i in tqdm(range(len(train_loader))):\n",
    "#     batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J67Ms2_jCfNk"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwwVKAh3AUx8"
   },
   "source": [
    "This is the baseline model for predicting purchases in `merchant_type` in the next 2 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kWquDGFCfNl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-PAEoqoCfNo"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'merchant_type_emb_dim': 64,\n",
    "    'trans_type_embedding': 3,\n",
    "    'transformer_nhead': 2,\n",
    "    'transformer_dim_feedforward': 256,\n",
    "    'transformer_dropout': 0.1,\n",
    "    'dense_unit': 256,\n",
    "    'num_layers': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NeAfj7h_CfNq",
    "outputId": "99271840-d606-4a6b-d976-a57c57663c0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MERCH_TYPE_NCLASSES, TRANS_TYPE_NCLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDhr4svJCfNs"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.merchant_type_embedding = nn.Embedding(\n",
    "            MERCH_TYPE_NCLASSES, params[\"merchant_type_emb_dim\"]\n",
    "        )\n",
    "        self.trans_type_embedding = nn.Embedding(\n",
    "            TRANS_TYPE_NCLASSES, params[\"trans_type_embedding\"]\n",
    "        )\n",
    "\n",
    "        embedding_size = (\n",
    "            params[\"merchant_type_emb_dim\"]\n",
    "            + params[\"trans_type_embedding\"]\n",
    "            + 1\n",
    "        )\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for i in range(params[\"num_layers\"]):\n",
    "            transformer_block = nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=params[\"transformer_nhead\"],\n",
    "                dim_feedforward=params[\"transformer_dim_feedforward\"],\n",
    "                dropout=params[\"transformer_dropout\"],\n",
    "            )\n",
    "            transformer_blocks.append(\n",
    "                (f\"transformer_block_{i}\", transformer_block)\n",
    "            )\n",
    "\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            OrderedDict(transformer_blocks)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_size, out_features=params[\"dense_unit\"]\n",
    "        )\n",
    "        self.scorer = nn.Linear(\n",
    "            in_features=params[\"dense_unit\"],\n",
    "            out_features=MERCH_TYPE_NCLASSES - 1,\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        merchant_type_emb = self.merchant_type_embedding(features[\"merchant_type\"])\n",
    "        trans_type_emb = self.trans_type_embedding(features[\"trans_type\"])\n",
    "\n",
    "        merchant_type_emb = merchant_type_emb * features[\"merchant_type_mask\"].unsqueeze(-1)\n",
    "        trans_type_emb = trans_type_emb * features[\"trans_type_mask\"].unsqueeze(-1)\n",
    "\n",
    "        embeddings = torch.cat(\n",
    "            (merchant_type_emb, trans_type_emb, features[\"sum\"].unsqueeze(-1)),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        pooling = torch.mean(transformer_output, dim=1)\n",
    "        linear = torch.tanh(self.linear(pooling))\n",
    "        merch_logits = self.scorer(linear)\n",
    "\n",
    "        return merch_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NszoLAhbCfNu"
   },
   "source": [
    "### One-batch-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PVdmq86KCfNu",
    "outputId": "a98d1094-7aaf-46ca-a49d-b731fe8a8127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7022, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "batch = next(iter(train_loader))\n",
    "output = model(batch['features'])\n",
    "loss = criterion(output, batch['targets'])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZz71tEDCfNw"
   },
   "source": [
    "## Train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
    "\n",
    "[A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcF4frWZCfNx"
   },
   "outputs": [],
   "source": [
    "from catalyst import dl, utils\n",
    "from catalyst.utils import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6EBSFvEwpQl"
   },
   "source": [
    "## Custom metrics for this hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OhGkjT0CfNz"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catalyst.utils.metrics.functional import preprocess_multi_label_metrics\n",
    "from catalyst.utils.torch import get_activation_fn\n",
    "\n",
    "\n",
    "def multi_label_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    threshold: Union[float, torch.Tensor],\n",
    "    activation: Optional[str] = None,\n",
    "    eps: float = 1e-7,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes multi-label precision for the specified activation and threshold.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "            indicates the probability of the example belonging to each of\n",
    "            the K classes, according to the model.\n",
    "        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "            classes are associated with the N-th input\n",
    "            (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "            associated with classes 2 and 4)\n",
    "        threshold (float): threshold for for model output\n",
    "        activation (str): activation to use for model output\n",
    "        eps (float): epsilon to avoid zero division\n",
    "    \n",
    "    Extended version of \n",
    "        https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/metrics/accuracy.py#L58\n",
    "\n",
    "    Returns:\n",
    "        computed multi-label metrics\n",
    "    \"\"\"\n",
    "    outputs, targets, _ = preprocess_multi_label_metrics(\n",
    "        outputs=outputs, targets=targets\n",
    "    )\n",
    "    activation_fn = get_activation_fn(activation)\n",
    "    outputs = activation_fn(outputs)\n",
    "\n",
    "    outputs = (outputs > threshold).long()\n",
    "\n",
    "    accuracy = (targets.long() == outputs.long()).sum().float() / np.prod(\n",
    "        targets.shape\n",
    "    )\n",
    "\n",
    "    intersection = (outputs.long() * targets.long()).sum(axis=1).float()\n",
    "    num_predicted = outputs.long().sum(axis=1).float()\n",
    "    num_relevant = targets.long().sum(axis=1).float()\n",
    "    union = num_predicted + num_relevant\n",
    "\n",
    "    # Precision = ({predicted items} && {relevant items}) / {predicted items}\n",
    "    precision = intersection / (num_predicted + eps * (num_predicted == 0))\n",
    "    # Recall = ({predicted items} && {relevant items}) / {relevant items}\n",
    "    recall = intersection / (num_relevant + eps * (num_relevant == 0))\n",
    "    # IoU = ({predicted items} && {relevant items}) / ({predicted items} || {relevant items})\n",
    "    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n",
    "\n",
    "    return accuracy, precision.mean(), recall.mean(), iou.mean()\n",
    "\n",
    "\n",
    "def precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "       actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "       predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       k (int): parameter k of precison@k\n",
    "\n",
    "    Returns:\n",
    "       Computed value of precision@k for given sample\n",
    "    \"\"\"\n",
    "    p_at_k = 0.0\n",
    "    for item in predicted[:k]:\n",
    "        if actual[item]:\n",
    "            p_at_k += 1\n",
    "    p_at_k /= k\n",
    "\n",
    "    return p_at_k\n",
    "\n",
    "\n",
    "def average_precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes average precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "      actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "      predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "          classes are associated with the N-th input\n",
    "          (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "          associated with classes 2 and 4)\n",
    "      k (int): parameter k of AP@k\n",
    "\n",
    "    Returns:\n",
    "        Computed value of AP@k for given sample\n",
    "    \"\"\"\n",
    "    ap_at_k = 0.0\n",
    "    for idx, item in enumerate(predicted[:k]):\n",
    "        if actual[item]:\n",
    "            ap_at_k += precision_at_k(actual, predicted, k=idx + 1)\n",
    "    ap_at_k /= min(k, actual.sum().cpu().numpy())\n",
    "    \n",
    "\n",
    "    return ap_at_k\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(\n",
    "    output: torch.Tensor, target: torch.Tensor, top_k: Tuple[int, ...] = (1,)\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes mean_average_precision_at_k at set of cutoff parameters K\n",
    "\n",
    "    Args:\n",
    "       outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "           indicates the probability of the example belonging to each of\n",
    "           the K classes, according to the model.\n",
    "       targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       top_k (tuple): list of parameters k at which map@k will be computed\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       List of computed values of map@k at each cutoff k from topk\n",
    "    \"\"\"\n",
    "    max_k = max(top_k)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, top_indices = output.topk(k=max_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "    result = []\n",
    "    for k in top_k:  # loop over k\n",
    "        map_at_k = 0.0\n",
    "        for actual_target, predicted_items in zip(\n",
    "            target, top_indices\n",
    "        ):  # loop over samples\n",
    "            map_at_k += average_precision_at_k(\n",
    "                actual_target, predicted_items, k\n",
    "            )\n",
    "        map_at_k = map_at_k / batch_size\n",
    "        result.append(map_at_k)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GygG0d1WCfN2"
   },
   "outputs": [],
   "source": [
    "# What is Runner?\n",
    "# https://catalyst-team.github.io/catalyst/api/core.html#runner\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        features, targets = batch[\"features\"], batch[\"targets\"]\n",
    "        logits = self.model(features)\n",
    "        scores = torch.sigmoid(logits)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "        accuracy, precision, recall, iou = multi_label_metrics(\n",
    "            logits, targets, threshold=0.5, activation=\"Sigmoid\"\n",
    "        )\n",
    "        map05, map10, map20 = mean_average_precision_at_k(\n",
    "            scores, targets, top_k=(5, 10, 20)\n",
    "        )\n",
    "        batch_metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"iou\": iou,\n",
    "            \"map05\": map05,\n",
    "            \"map10\": map10,\n",
    "            \"map20\": map20\n",
    "        }\n",
    "        \n",
    "        self.input = {\"features\": features, \"targets\": targets}\n",
    "        self.output = {\"logits\": logits, \"scores\": scores}\n",
    "        self.batch_metrics.update(batch_metrics)\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def predict_batch(self, batch):\n",
    "        # model inference step\n",
    "        batch = utils.maybe_recursive_call(batch, \"to\", device=self.device)\n",
    "        logits = self.model(batch[\"features\"])\n",
    "        scores = torch.sigmoid(logits)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noTAPVPSCfN5"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CH-bnRkkSRYA"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "id": "mhxJPaHTCfN6",
    "outputId": "f761ac29-ebf8-4292-e6ff-a1b99c524774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train): 100% 3258/3258 [20:05<00:00,  2.70it/s, accuracy=0.984, iou=0.260, loss=0.047, map05=0.585, map10=0.536, map20=0.518, precision=0.693, recall=0.307]\n",
      "1/10 * Epoch (valid): 100% 253/253 [01:24<00:00,  2.99it/s, accuracy=0.983, iou=0.280, loss=0.056, map05=0.589, map10=0.451, map20=0.399, precision=0.665, recall=0.370]\n",
      "[2020-09-17 11:36:25,117] \n",
      "1/10 * Epoch 1 (train): accuracy=0.9836 | iou=0.2356 | loss=0.0504 | map05=0.5561 | map10=0.5043 | map20=0.4941 | precision=0.6421 | recall=0.2976\n",
      "1/10 * Epoch 1 (valid): accuracy=0.9845 | iou=0.2979 | loss=0.0472 | map05=0.6398 | map10=0.5723 | map20=0.5515 | precision=0.6590 | recall=0.3923\n",
      "2/10 * Epoch (train): 100% 3258/3258 [20:14<00:00,  2.68it/s, accuracy=0.988, iou=0.328, loss=0.040, map05=0.621, map10=0.574, map20=0.573, precision=0.695, recall=0.450]\n",
      "2/10 * Epoch (valid): 100% 253/253 [01:24<00:00,  2.98it/s, accuracy=0.982, iou=0.282, loss=0.054, map05=0.604, map10=0.480, map20=0.442, precision=0.631, recall=0.377]\n",
      "[2020-09-17 11:58:04,786] \n",
      "2/10 * Epoch 2 (train): accuracy=0.9850 | iou=0.2982 | loss=0.0454 | map05=0.6499 | map10=0.5862 | map20=0.5681 | precision=0.6889 | recall=0.3823\n",
      "2/10 * Epoch 2 (valid): accuracy=0.9852 | iou=0.3384 | loss=0.0453 | map05=0.6858 | map10=0.6188 | map20=0.5960 | precision=0.7003 | recall=0.4466\n",
      "3/10 * Epoch (train): 100% 3258/3258 [20:23<00:00,  2.66it/s, accuracy=0.986, iou=0.333, loss=0.044, map05=0.692, map10=0.624, map20=0.591, precision=0.732, recall=0.413]\n",
      "3/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.94it/s, accuracy=0.984, iou=0.292, loss=0.053, map05=0.620, map10=0.522, map20=0.450, precision=0.646, recall=0.396]\n",
      "[2020-09-17 12:19:54,062] \n",
      "3/10 * Epoch 3 (train): accuracy=0.9855 | iou=0.3264 | loss=0.0440 | map05=0.6802 | map10=0.6166 | map20=0.5965 | precision=0.7118 | recall=0.4220\n",
      "3/10 * Epoch 3 (valid): accuracy=0.9855 | iou=0.3469 | loss=0.0442 | map05=0.7044 | map10=0.6377 | map20=0.6127 | precision=0.7131 | recall=0.4549\n",
      "4/10 * Epoch (train): 100% 3258/3258 [20:24<00:00,  2.66it/s, accuracy=0.986, iou=0.335, loss=0.044, map05=0.729, map10=0.624, map20=0.617, precision=0.689, recall=0.510]\n",
      "4/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.92it/s, accuracy=0.982, iou=0.272, loss=0.053, map05=0.612, map10=0.527, map20=0.451, precision=0.578, recall=0.387]\n",
      "[2020-09-17 12:41:45,111] \n",
      "4/10 * Epoch 4 (train): accuracy=0.9857 | iou=0.3419 | loss=0.0432 | map05=0.6932 | map10=0.6311 | map20=0.6106 | precision=0.7174 | recall=0.4447\n",
      "4/10 * Epoch 4 (valid): accuracy=0.9855 | iou=0.3688 | loss=0.0436 | map05=0.7094 | map10=0.6458 | map20=0.6219 | precision=0.6783 | recall=0.5162\n",
      "5/10 * Epoch (train): 100% 3258/3258 [20:34<00:00,  2.64it/s, accuracy=0.985, iou=0.347, loss=0.045, map05=0.697, map10=0.605, map20=0.586, precision=0.697, recall=0.449]\n",
      "5/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.91it/s, accuracy=0.983, iou=0.291, loss=0.053, map05=0.622, map10=0.524, map20=0.442, precision=0.621, recall=0.401]\n",
      "[2020-09-17 13:03:46,952] \n",
      "5/10 * Epoch 5 (train): accuracy=0.9858 | iou=0.3507 | loss=0.0427 | map05=0.7000 | map10=0.6393 | map20=0.6188 | precision=0.7172 | recall=0.4587\n",
      "5/10 * Epoch 5 (valid): accuracy=0.9857 | iou=0.3683 | loss=0.0429 | map05=0.7175 | map10=0.6544 | map20=0.6295 | precision=0.7000 | recall=0.4998\n",
      "6/10 * Epoch (train): 100% 3258/3258 [20:48<00:00,  2.61it/s, accuracy=0.987, iou=0.350, loss=0.040, map05=0.707, map10=0.642, map20=0.629, precision=0.718, recall=0.476]\n",
      "6/10 * Epoch (valid): 100% 253/253 [01:26<00:00,  2.91it/s, accuracy=0.984, iou=0.315, loss=0.052, map05=0.622, map10=0.538, map20=0.471, precision=0.617, recall=0.428]\n",
      "[2020-09-17 13:26:01,960] \n",
      "6/10 * Epoch 6 (train): accuracy=0.9859 | iou=0.3567 | loss=0.0423 | map05=0.7041 | map10=0.6445 | map20=0.6243 | precision=0.7173 | recall=0.4683\n",
      "6/10 * Epoch 6 (valid): accuracy=0.9858 | iou=0.3720 | loss=0.0428 | map05=0.7149 | map10=0.6537 | map20=0.6288 | precision=0.6953 | recall=0.5087\n",
      "7/10 * Epoch (train): 100% 3258/3258 [20:40<00:00,  2.63it/s, accuracy=0.986, iou=0.345, loss=0.041, map05=0.701, map10=0.652, map20=0.630, precision=0.701, recall=0.454]\n",
      "7/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.89it/s, accuracy=0.984, iou=0.301, loss=0.050, map05=0.649, map10=0.569, map20=0.487, precision=0.620, recall=0.410]\n",
      "[2020-09-17 13:48:09,658] \n",
      "7/10 * Epoch 7 (train): accuracy=0.9860 | iou=0.3607 | loss=0.0420 | map05=0.7067 | map10=0.6482 | map20=0.6281 | precision=0.7165 | recall=0.4746\n",
      "7/10 * Epoch 7 (valid): accuracy=0.9858 | iou=0.3783 | loss=0.0426 | map05=0.7250 | map10=0.6638 | map20=0.6399 | precision=0.6921 | recall=0.5213\n",
      "8/10 * Epoch (train): 100% 3258/3258 [20:48<00:00,  2.61it/s, accuracy=0.987, iou=0.385, loss=0.040, map05=0.710, map10=0.663, map20=0.637, precision=0.693, recall=0.547]\n",
      "8/10 * Epoch (valid): 100% 253/253 [01:28<00:00,  2.86it/s, accuracy=0.985, iou=0.336, loss=0.050, map05=0.641, map10=0.538, map20=0.464, precision=0.627, recall=0.452]\n",
      "[2020-09-17 14:10:27,181] \n",
      "8/10 * Epoch 8 (train): accuracy=0.9860 | iou=0.3634 | loss=0.0418 | map05=0.7088 | map10=0.6510 | map20=0.6313 | precision=0.7159 | recall=0.4795\n",
      "8/10 * Epoch 8 (valid): accuracy=0.9858 | iou=0.3795 | loss=0.0424 | map05=0.7213 | map10=0.6626 | map20=0.6379 | precision=0.6852 | recall=0.5271\n",
      "9/10 * Epoch (train): 100% 3258/3258 [20:37<00:00,  2.63it/s, accuracy=0.985, iou=0.351, loss=0.044, map05=0.704, map10=0.658, map20=0.627, precision=0.711, recall=0.484]\n",
      "9/10 * Epoch (valid): 100% 253/253 [01:27<00:00,  2.90it/s, accuracy=0.984, iou=0.303, loss=0.052, map05=0.610, map10=0.538, map20=0.461, precision=0.615, recall=0.420]\n",
      "[2020-09-17 14:32:32,102] \n",
      "9/10 * Epoch 9 (train): accuracy=0.9860 | iou=0.3662 | loss=0.0416 | map05=0.7102 | map10=0.6533 | map20=0.6335 | precision=0.7152 | recall=0.4838\n",
      "9/10 * Epoch 9 (valid): accuracy=0.9859 | iou=0.3771 | loss=0.0422 | map05=0.7231 | map10=0.6651 | map20=0.6419 | precision=0.6996 | recall=0.5110\n",
      "10/10 * Epoch (train): 100% 3258/3258 [20:59<00:00,  2.59it/s, accuracy=0.986, iou=0.380, loss=0.040, map05=0.748, map10=0.697, map20=0.648, precision=0.681, recall=0.511]\n",
      "10/10 * Epoch (valid): 100% 253/253 [01:28<00:00,  2.87it/s, accuracy=0.984, iou=0.288, loss=0.051, map05=0.611, map10=0.503, map20=0.452, precision=0.551, recall=0.359]\n",
      "[2020-09-17 14:54:59,952] \n",
      "10/10 * Epoch 10 (train): accuracy=0.9861 | iou=0.3679 | loss=0.0414 | map05=0.7114 | map10=0.6548 | map20=0.6355 | precision=0.7145 | recall=0.4869\n",
      "10/10 * Epoch 10 (valid): accuracy=0.9858 | iou=0.3814 | loss=0.0423 | map05=0.7228 | map10=0.6638 | map20=0.6404 | precision=0.6859 | recall=0.5298\n",
      "Top best models:\n",
      "logs/checkpoints/train.10.pth\t0.3814\n",
      "=> Loading checkpoint logs/checkpoints/best_full.pth\n",
      "loaded state checkpoint logs/checkpoints/best_full.pth (global epoch 10, epoch 10, stage train)\n"
     ]
    }
   ],
   "source": [
    "# For other minimal examples, please follow the link below\n",
    "# https://github.com/catalyst-team/catalyst#minimal-examples\n",
    "runner = CustomRunner()\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True,\n",
    "    overfit=False,  #  <<<--- DO NOT FORGET TO MAKE IT ``False`` \n",
    "                    #  (``True`` uses only one batch to check pipeline correctness)\n",
    "    callbacks=[\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "        # dl.AveragePrecisionCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"ap\"),\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "        # dl.AUCCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"auc\"),\n",
    "    ],\n",
    "    main_metric=\"iou\", # \"ap/mean\", \n",
    "    minimize_metric=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQxuxPukCfN9"
   },
   "outputs": [],
   "source": [
    "# model inference example\n",
    "# for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
    "#     assert prediction.detach().cpu().numpy().shape[-1] == MERCH_TYPE_NCLASSES-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFV8lYM9CfN_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import plotly  # required for contrib\n",
    "# from catalyst.contrib.utils import plot_tensorboard_log\n",
    "\n",
    "# plot_tensorboard_log(logdir=\"./logs\", step=\"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bte9iiW0CfOB"
   },
   "outputs": [],
   "source": [
    "import plotly  # required for contrib\n",
    "from catalyst.contrib.utils import plot_tensorboard_log\n",
    "\n",
    "plot_tensorboard_log(\n",
    "    logdir=\"./logs\", \n",
    "    step=\"epoch\", \n",
    "    metrics=[\n",
    "        \"loss\", \"accuracy\", \"precision\", \"recall\", \"iou\", \n",
    "        \"map05\", \"map10\", \"map20\",\n",
    "        \"ap/mean\", \"auc/mean\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOBi1DqJPt_F"
   },
   "source": [
    "## Your task\n",
    "\n",
    "We suggest you to improve this baseline. Feel free to use any kind of model architectures, loss functions, inputs, etc. in your experiments.\n",
    "\n",
    "\n",
    "YOUR TASK is to predict purchases in `merchant_type` in **January-February 2020** for all the clients (50k) from the given dataset.\n",
    "\n",
    "SUBMISSION FORMAT: You should submit a `.csv` file in the following format. \n",
    "\n",
    "The submission file should contain two columns:\n",
    "* `party_rk` -- client unique identifier\n",
    "* `recommendations` -- list of the **top 30** predicted `merchant_type`, sorted by predicted proba (pay attention!) **separated by commas**. \n",
    "\n",
    "The `.csv` file separator should be **semicolon (\";\")**. The submission file example can be generated by the pipeline shown below.\n",
    "\n",
    "EVALUATION: Your submission will be evaluated by metric **MAP@30**. Scores for this part of the hackathon will be given according to the value of this metric.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-30pWSiQL7vY"
   },
   "source": [
    "## Submission file example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylgn_EIXMQrr"
   },
   "outputs": [],
   "source": [
    "# create data loader for submission\n",
    "full_party = pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique()\n",
    "full_sum, full_trans_type, full_merchant_type, full_labels = prepare_data(\n",
    "    full_party, mode=\"submission\"\n",
    ")\n",
    "full_dataset = RSDataset(\n",
    "   full_sum, full_trans_type, full_merchant_type, full_labels\n",
    ")\n",
    "full_loader = DataLoader(\n",
    "    full_dataset, batch_size=64, shuffle=False, num_workers=8, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tdEPJ0gMahr"
   },
   "outputs": [],
   "source": [
    "# get predictions from the model\n",
    "predictions = []\n",
    "for scores in tqdm(runner.predict_loader(loader=full_loader), total = len(full_loader)):\n",
    "    _, top_indices = scores.topk(k=30, dim=1, largest=True, sorted=True)\n",
    "    top_indices += 1\n",
    "    predictions += top_indices.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZhfVRYmMpdr"
   },
   "outputs": [],
   "source": [
    "# inverse mapping for merchant_type in predictions\n",
    "merchant_type_inverse_mapping = {k: v for v, k in mappings['merchant_type'].items()}\n",
    "def inverse_mapping(x):\n",
    "    return list(map(merchant_type_inverse_mapping.get, x))\n",
    "\n",
    "predictions = list(map(inverse_mapping, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_CjmgByL9Vy"
   },
   "outputs": [],
   "source": [
    "# create submission table\n",
    "submission = pd.DataFrame({\n",
    "    \"party_rk\" : full_party, \n",
    "    \"recommendations\" : predictions\n",
    "})\n",
    "submission['recommendations'] = submission['recommendations'].apply(lambda x: \",\".join(map(str, x)))\n",
    "\n",
    "submission.to_csv('submission_[TEAM_NAME].csv', index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2020_hack_baseline_public.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
